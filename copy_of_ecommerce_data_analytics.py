# -*- coding: utf-8 -*-
"""Copy of Ecommerce_Data_Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DTqcjwA_DQNbJqjRAVNrPt3v58oih0yO

Olistic Comapany, Data is stored in PostgreSQL

# **Download the data from kaggle**

First we will install kaggle python library in the colab notebook
"""

!pip install kaggle

!ls -a

"""Make .kaggle folder in the /content folder of colab"""

!mkdir .kaggle

"""Below you have to provide your kaggle user name and kaggle API key.  Follow these instructions to create an API key: http://bit.ly/kaggle-creds

"""

import json
kaggle_username = input('Enter Kaggle username:')
kaggle_apikey = input('Enter Kaggle API key:')
token = {"username":kaggle_username,"key":kaggle_apikey}
with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(token, file)

!chmod 600 /content/.kaggle/kaggle.json

# this will give an error the first time you run it but that is no issues
# just run and then run the below cell
!kaggle config set -n path -v{/content}

!cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json

!cd ~

!pwd

!kaggle datasets download -d olistbr/brazilian-ecommerce -p /content

!unzip \*.zip

"""Now the datasets will be available in the /content folder of colab

# **Business and Data Understanding**

**Business Understanding**

This is a real-world dataset coming from the brazilian ecommerce company Olist.
 
Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on our website: www.olist.com

After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.

Attention
* An order might have multiple items.
* Each item might be fulfilled by a distinct seller.
* All text identifying stores and partners where replaced by the names of Game of Thrones great houses.

**Data Schema and Understanding**

The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers.

The data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:

![](https://drive.google.com/uc?export=view&id=1Wyr2o4YCtdo5USupfTioipApit-Ui9Cw)

# **Problem Statements**

The notebook will present 4 goals of analysis to help more understand about Olist performance as below, more or less almost all ecommerce businesses have to solve these use cases:

**Goal 1. Business Overview**

**Goal 2. Peak time of Sales**

**Goal 3. Orders'Delivery Performance**

**Goal 4. Customer Segmentation Analysis using RFM methodology**

# **Data Import, Preparation and Basic Analysis**

## Data Import

1. import neccessary liberary
2. read csv

A1. Import neccessary liberary
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""A2. Read csv files"""

file_name ={'orders':'olist_orders_dataset', 'order_items':'olist_order_items_dataset', 'products':'olist_products_dataset',
            'eng_product_cate':'product_category_name_translation','customers': 'olist_customers_dataset','sellers': 'olist_sellers_dataset',
            'payments':'olist_order_payments_dataset', 'reviews':'olist_order_reviews_dataset', 'geolocation':'olist_geolocation_dataset'}
dataframes_dict = {}
for name in file_name.keys():
    dataframes_dict[name] = pd.read_csv(file_name[name]+'.csv')

"""## Data Understanding

Evaluate the data in dataset (columns, NaN value...)
"""

###list down all datasets###
dataset_name = file_name.keys()
dataset_name

###list down all columns in dataset###
df_columns_name= pd.DataFrame([dataframes_dict[i].columns for i in dataset_name], index = dataset_name).T
df_columns_name

"""Below we are showing how to get number of NaNs value, number of unique values and data type of each column of the dataset `order_items`. We will get all this information into a new dataframe"""

df_order_items = dataframes_dict['order_items']

print(df_order_items.nunique())   # number of unique values in each column
print(df_order_items.dtypes)      # data type of each columns
print(df_order_items.isna().sum()*100/len(df_order_items)) # number of NaNs value in each column

df1_order_items = pd.DataFrame([df_order_items.nunique(), df_order_items.dtypes, \
                                df_order_items.isna().sum()*100/len(df_order_items)], index=['unique_entries','dtype','NaN_pt(%)'])

df1_order_items

df1_order_items_transposed = df1_order_items.T
df1_order_items_transposed

# in the above dataset column names are indexes we will convert it into a column
df1_order_items_transposed.reset_index(inplace = True)
df1_order_items_transposed

df1_order_items_transposed['dataset'] = 'order_items'
df1_order_items_transposed

"""This is how the data should look like for each dataset but it is not feasible to do this again and again for each dataset, so below we have created a function which will take all the datasets and do this analysis in one go."""

### Define which datasets using in this scope and study about their columns###
pd.set_option('precision', 2)
dataset_list = ['orders','order_items', 'products', 'eng_product_cate', 'customers','reviews','sellers']

# define a function to get basic info (unique entries, %NaN value, dtype) of each column in the dataset 
def dataset_info(df, df_name):
    df1 = pd.DataFrame([df.nunique(), df.dtypes, df.isna().sum()*100/len(df)], index=['unique_entries','dtype','NaN_pt(%)']).T
    df1.reset_index(inplace = True)
    df1['dataset_name'] = df_name
    return df1

#define a function to highlight NaN_pt(%) > 0:
def highlight_NaN(x):
    return ['color:darkblue;background-color:pink' if v > 0 else '' for v in x]

#use loop for to go through all using datasets
df_dataset_info = pd.DataFrame()
for dataset in dataset_list: 
    df2 = dataset_info(dataframes_dict[dataset], dataset)
    df_dataset_info = df_dataset_info.append(df2)

df_dataset_info.reset_index(drop=True, inplace=True)

# .style method applies the styling function 
df_dataset_format = df_dataset_info.style.apply(highlight_NaN, subset=pd.IndexSlice[:, ["NaN_pt(%)"]])
df_dataset_format

"""**Conclusion:**

* Most of columns dont have much NaN value, except 2 columns: "review comment title" and "review comment message" in dataset "reviews"

## Data Preparation - Orders data
"""

### processing data in dataset: "orders" ###

#Add some timing columns for "orders" dataset (year, month, day of week, time of day...)

#define a function for time_period with:
#  Morning = 5:00 to before 12:00
#  Afternoon = 12:00 to before 17:00
#  Evening = 17:00 to before 21:00
#  Night = 21:00 to before 4:00

def time_period(x):
    if x>=5 and x<12:
        return "Morning"
    elif x>=12 and x<17:
        return "Afternoon"
    elif x>=17 and x<21:
        return "Evening"
    else:
        return "Night"

#get neccessary columns from "orders" dataset:
orders_df = dataframes_dict['orders'][['order_id','customer_id','order_status','order_purchase_timestamp','order_estimated_delivery_date',
                    'order_delivered_customer_date','order_delivered_carrier_date']]

#assign new columns about year, month, day of week, time of day...
orders_df = orders_df.assign(
    purchase_date = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.date,
    purchase_year = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.year,
    purchase_month = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.month,
    purchase_MMYYYY= pd.to_datetime(orders_df['order_purchase_timestamp']).dt.strftime('%b-%y'),
    purchase_day = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.day_name(),
    purchase_hour = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.hour)

#apply the defined "time_period" function:
orders_df["purchase_time"]= orders_df["purchase_hour"].apply(time_period)

orders_df

###calculate %orders by status for each year###

#define function to format chart (re-use for further charts):
def format_chart(ax):
    ax.title.set_size(14)
    ax.xaxis.label.set_size(13)
    ax.yaxis.label.set_size(13)
    ax.tick_params(labelsize=11)

#create pivot table to count orders by status and year, replace NaN with 0:
ord_stt = orders_df.pivot_table(values = 'order_id', index='order_status'
                                , columns='purchase_year', aggfunc= 'count')
ord_stt.fillna(0, inplace=True)

#calculate %order by status for each year:
ord_stt = ord_stt.apply(lambda x: ((x*100)/x.sum()).round(2), axis=0).T

#format dataframe's caption, tbody, th:
styles = [dict(selector="caption",
            props=[("text-align", "center"),
                   ("font-size", "110%"),
                   ("color", 'darkblue'),
                  ("font-weight", "bold")]),
          dict(selector="tbody",
            props=[("color", 'darkblue')]),
         dict(selector="th",
            props=[("color", 'white'),
                  ('background-color','#6E7894')])]

ord_st=ord_stt.style.set_caption('Percentage of Orders by Status').set_table_styles(styles)

# present data to heatmap:
fig, ax = plt.subplots(figsize=(10,5))
ax=sns.heatmap(data=ord_stt, cmap='Blues', linecolor="white", linewidths=0.5, vmin=0, vmax=80, 
               annot=True, annot_kws={"size":12});
ax.set_title("Percentage of Orders by Status")

#call "format_chart_small" function to format chart:
format_chart(ax)
# set fontsize for cbar: use matplotlib.colorbar object, then set labelsize
cbar = ax.collections[0].colorbar
cbar.ax.tick_params(labelsize=11)
plt.savefig('Percentage of Orders by Status', dpi=400, bbox_inches='tight');

ord_st

"""**Conclusion:**
* It can be seen, for all reported years, percentage of orders under "delivered" status is  accounted for largest proportion (over 80%)

### Resized scope of work: 

Because more than 80% orders are under "delivered" status, and we cannot know clearly other status's meaning in Olist company, therefore, determine to analyze only "delivered" status in the scope of this analysis.
In this scope also consider "price" as revenue of the products (ignore freight value).

# **Goal analysis**

Before doing analysis, merge the neccessary datasets into 1 dataframe
"""

### Merging neccessary datasets for analysis ###

detail_df= (((dataframes_dict['order_items'].merge(orders_df, how="left",on='order_id'))
                 .merge(dataframes_dict['products'], how="left",on='product_id'))
            .merge(dataframes_dict['eng_product_cate'], how='left', on='product_category_name'))\
            .merge(dataframes_dict['customers'], how="left", on="customer_id")

# create a new column with condition: 
conditions = [(detail_df['purchase_day'] == 'Saturday'),(detail_df['purchase_day'] == 'Sunday')]
choices = ['weekends', 'weekends']
detail_df['weekday'] = np.select(conditions, choices, default='workdays')

#filter order_status == "delivered" (because only analyze delivered orders):
detail_df= detail_df[detail_df['order_status']=='delivered']

detail_df

#pick up neccessary columns for analysis:
products_df = detail_df[['order_id', 'product_id','price', 'order_status', 'purchase_date','purchase_MMYYYY', 'purchase_year','purchase_month','purchase_day','purchase_time','weekday','product_category_name_english', 'customer_unique_id', 'customer_state'
                         , 'order_delivered_customer_date', 'order_estimated_delivery_date','order_delivered_carrier_date','shipping_limit_date', 'seller_id']]

products_df

"""Now, we start to analysis the first goal "Business Overview"

## **1. Business Overview**

In this part,we will learn about the overall Olist business on term of revenue, number of orders, order size (value of order), top product categories... for general assessment

### Revenue and Order Trend
"""

### Order Trend by Month ###
#set theme for all charts:
plt.style.use("seaborn-whitegrid")
sns.set_theme(style="whitegrid", palette="pastel")

#define a function to format chart:
def format_chart_trend(ax):
    ax.title.set_size(22)
    ax.xaxis.label.set_size(17)
    ax.yaxis.label.set_size(17)
    ax.xaxis.set_tick_params(labelsize=14)
    ax.yaxis.set_tick_params(labelsize=14)
    
#create df order_M to calculate no of delivered orders by month:    
ord_M=products_df.pivot_table(values = ['order_id', 'price']
                              , index=['purchase_year','purchase_month','purchase_MMYYYY']
                              , aggfunc={'order_id':'nunique','price':'sum'})

#sort data by timing (for purchase_MMYYY col):
ord_M = ord_M.sort_index(ascending=[1,1,1])
ord_M.reset_index(inplace = True)
del ord_M['purchase_year']
del ord_M['purchase_month']
ord_M.set_index('purchase_MMYYYY', inplace=True)
ord_M['revenue($R1000)']=ord_M['price']/1000
del ord_M['price']

#present data to line chart:
width = .45
fig = plt.figure()
ax1 = ord_M['revenue($R1000)'].plot(kind='bar', figsize=(20,7), width = width)
ax2 = ord_M['order_id'].plot(secondary_y=True, color='#007FD1')
ax1.set(ylabel='revenue ($R1000)')
ax1.set(title="Trend of Revenue ($R1000) & Orders by Month", xlabel="purchased month")
ax1.title.set_size(18)
ax1.xaxis.label.set_size(14)
ax1.yaxis.label.set_size(14)
ax1.xaxis.set_tick_params(labelsize=13)
ax1.yaxis.set_tick_params(labelsize=13)
ax2.yaxis.label.set_size(14)
ax2.yaxis.set_tick_params(labelsize=13)
ax2.set_ylabel('no of orders', rotation=-90, labelpad=20) 
fig.legend(loc='upper right', fontsize=14)

#annotation of the peak point:
from datetime import datetime
peak = 'Peak of revenue & orders'
ax2.annotate(peak, xy=(13, 7289+50),
             xytext=(13, 7289 + 300),fontsize=15, color='red',
             arrowprops=dict(facecolor='#FC5190',shrink=0.05),
             horizontalalignment='left', verticalalignment='top')
plt.savefig('Trend of Revenue ($R1000) & Orders by Month', dpi=400, bbox_inches='tight');

#8M2018 yoy growth:
products_8M=products_df[products_df['purchase_year'].isin([2017,2018])]
products_8M=products_8M[products_8M['purchase_month']<=8]
products_8M['revenue($R1000)']=products_8M['price']/1000
products_8M=products_8M.pivot_table(values='revenue($R1000)', columns='purchase_year', aggfunc ='sum')
products_8M.reset_index(inplace=True)
products_8M['%Growth']=(products_8M[2018]/products_8M[2017]-1)*100


products_8M.set_index('index',inplace=True)

products_8M.columns.names = ['']
products_8M.index.names = ['']
# print(products_8M.columns)
# print(products_8M.index.names)

product_8M_style = products_8M.style.applymap(
    lambda x: 'color:darkblue;background-color:pink' if x==products_8M.iloc[0,2] else '')
product_8M_style

### Order Trend in Nov-17 ###
#because Nov-17 has the highest revenue and orders, we dig down the data by date in Nov-17 to see the reason
#set theme for all charts:
plt.style.use("seaborn-whitegrid")
sns.set_theme(style="whitegrid", palette="pastel")

#define a function to format chart:
def format_chart_trend(ax):
    ax.title.set_size(22)
    ax.xaxis.label.set_size(17)
    ax.yaxis.label.set_size(17)
    ax.xaxis.set_tick_params(labelsize=14)
    ax.yaxis.set_tick_params(labelsize=14)
    
#create df order_M to calculate no of delivered orders by month:    
ord_Nov=products_df[products_df['purchase_MMYYYY']== 'Nov-17'].pivot_table(values = ['order_id', 'price']
                              , index=['purchase_date']
                              , aggfunc={'order_id':'nunique','price':'sum'})

#change purchase_date to str to draw column, line chart:
ord_Nov.sort_index(ascending=True, inplace=True)
ord_Nov.reset_index(inplace = True)
ord_Nov = ord_Nov.astype({"purchase_date": str}, errors='raise') 
ord_Nov.set_index('purchase_date', inplace=True)
ord_Nov['revenue($R1000)']=ord_Nov['price']/1000
#del ord_Nov['price']
ord_Nov
#present data to line chart:
width = .45
fig = plt.figure(figsize=(20,7))
ax1 = ord_Nov['revenue($R1000)'].plot(kind='bar',width = width)
ax2 = ord_Nov['order_id'].plot(kind='line',secondary_y=True, color='#007FD1')
ax1.set(ylabel='revenue ($R1000)')
ax1.set(title="Trend of Revenue ($R1000) & Orders in Nov-17", xlabel="purchased month")
ax1.title.set_size(18)
ax1.xaxis.label.set_size(14)
ax1.yaxis.label.set_size(14)
ax1.xaxis.set_tick_params(labelsize=13, rotation=90)
ax1.yaxis.set_tick_params(labelsize=13)

ax2.yaxis.label.set_size(14)
ax2.yaxis.set_tick_params(labelsize=13)
ax2.set_ylabel('no of orders', rotation=-90, labelpad=20) 
fig.legend(loc='upper right', fontsize=14)

#annotation of the peak point:
peak = 'Peak of revenue & orders'
ax2.annotate(peak, xy=(23, 1147),
             xytext=(23+0.5, 1147 + 50),fontsize=15, color='red',
             arrowprops=dict(facecolor='#FC5190',shrink=0.05),
             horizontalalignment='left', verticalalignment='top')
plt.savefig('Trend of Revenue ($R1000) & Orders in Nov-17', dpi=400, bbox_inches='tight');

"""**Conclusion:**
* Revenue and no of orders sold of Olist business grow up well from Q3-2017 and reached the peak at Nov-17.
* Compared revenue 8M2018 with 8M2017, the growth rate is more than 140%. It is the rapid growth.
* In Nov-17, 11/24/2017 is the highest revenue date due to Black Friday.

=> Can conclude that Olist business is the upgrowing business with the speed of growth is fast and business will get the peak in special sale date (like Black Friday). Thus, in the special sale days, Olist needs to care about the traffic to their site, delivery performance. Retailers also need to care about their stocks, packaging process.

### Daily Orders and Order Size (Order Value)
"""

### Daily Orders and Order Size Range ###
#summarize no of orders, revenue, order size by date:
ord_daily = products_df.pivot_table(values=["order_id","price"], index=['purchase_date','weekday']
                                ,aggfunc={"order_id":'nunique', "price":"sum"})
ord_daily['order_size'] = ord_daily['price']/ord_daily['order_id']


#present data to charts:
fig, ax = plt.subplots(1,2, figsize=(12,5))
sns.histplot(ax=ax[0], data=ord_daily, x="order_id", binwidth=50)
ax[0].set(title='Daily No Of Orders Range', ylabel='no of days', xlabel='no of orders')

sns.histplot(ax=ax[1], data=ord_daily, x="order_size", binwidth=20, color='#F9AE96')
ax[1].set(title='Daily Order Size Range', ylabel='no of days', xlabel='order size')

format_chart(ax[0])
format_chart(ax[1])
plt.savefig('Daily Orders Range', dpi=400, bbox_inches='tight');

"""**Conclusion:**

* Daily no of orders are running in the large range up to 1,200 orders per day. 
* However, the most common no of orders purchased in a day is from 50 to 200 orders.
* Daily order size mostly is from "R$120" to "R$160".

### By top product category
"""

###Top 30 product categories ###

#calculate revenue, no of orders, order size by top product categories:
prod_cat=products_df.pivot_table(values=['price', 'order_id'], index=['product_category_name_english']
                          , aggfunc={'price': 'sum', 'order_id': 'nunique'})
prod_cat["ord_size($R)"]=prod_cat["price"]/prod_cat["order_id"]
prod_cat["price"]=prod_cat["price"]/1000
prod_cat.sort_values(by='price', ascending = False, inplace = True)
prod_cat_top=prod_cat.rename(columns={'order_id':'no_of_order','price':"revenue($R1000)"}).head(30)

#present data to bar plots:
fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 8), sharey=True)
fig.suptitle('Analysis By Product Category', fontsize=15)

sns.barplot(ax=ax[0], x='revenue($R1000)', y= prod_cat_top.index, data = prod_cat_top)
ax[0].set_title('Revenue ($R1000)')
ax[0].set_ylabel('product category')

sns.barplot(ax=ax[1], x='no_of_order', y = prod_cat_top.index, data = prod_cat_top)
ax[1].set_title('No Of Orders')

sns.barplot(ax=ax[2], x='ord_size($R)', y = prod_cat_top.index, data = prod_cat_top)
ax[2].set_title('Order Size ($R)')

for i in range(0,3):
    ax[i].set(xlabel=None)

for i in range(1,3):
    ax[i].set(ylabel=None)

#call defined function "format_chart_small" to format charts:
format_chart(ax[0])
format_chart(ax[1])
format_chart(ax[2])
plt.savefig('Analysis By Product Category', dpi=400, bbox_inches='tight');

"""**Conclusion:**

* Top 5 categories bring revenue are: health/beauty, watches/gifts, bed/bath/table, sports/leisure & computer/accessories
* Order size of these top categories is not high (just around "$R200").
* It means that Olist revenue doesn’t come from expensive products, it is from large volumes of order sold rather than.

## **2. Peak Time of sale**

To understand which day and time having great order sold.
So that both Olist and retailers can optimize their business strategy like considering suitable time to launch promotion campaign or new products.

Note:
* Morning = 5:00 to before 12:00
* Afternoon = 12:00 to before 17:00
* Evening = 17:00 to before 21:00
* Night = 21:00 to before 4:00

### Analyze Order By Day and Time
"""

###Daily Avg Orders and PO Size of workdays vs weekends###
#calculate avg daily no of orders and order size by workdays & weekends
ord_wd = ord_daily.pivot_table(values=['order_id','order_size'], index='weekday', aggfunc='mean')
ord_wd=ord_wd.reindex(index= ['workdays', 'weekends'])

#present data to barplot:
fig, ax = plt.subplots(1,2, figsize=(10,5))
fig.suptitle("Order analysis: workdays vs weekends", fontsize=15)
sns.barplot(x=ord_wd.index, y='order_id', data=ord_wd, ax=ax[0])
ax[0].set(title="Daily Avg No of Orders", xlabel=None, ylabel="no_of_orders")

sns.barplot(x=ord_wd.index, y='order_size', data=ord_wd, ax=ax[1])
ax[1].set(title="Daily Avg Order Size", xlabel=None)

#annotation value in columns:
for p in ax[0].patches:
    ax[0].annotate(format(p.get_height(), '.0f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 5), 
                   textcoords = 'offset points')
    
for p in ax[1].patches:
    ax[1].annotate(format(p.get_height(), '.0f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 5), 
                   textcoords = 'offset points')

format_chart(ax[0])
format_chart(ax[1])
plt.savefig('Order analysis: workdays vs weekends', dpi=400, bbox_inches='tight');

### Daily no of orders by time ###
#count no of orders by date and time of day:
ord_time=products_df.pivot_table(values="order_id", index=['purchase_date','purchase_time'],aggfunc="nunique")
ord_time.reset_index(level="purchase_date", inplace=True)
ord_time.reset_index(level="purchase_time", inplace=True)

#present data to boxplot:
fig, ax = plt.subplots(figsize=(6,7))
sns.boxplot(data=ord_time, x="purchase_time", y="order_id",order=["Morning","Afternoon","Evening","Night"])
ax.set(title="Daily No of Orders by Time", xlabel=None, ylabel='daily purchased orders')
format_chart(ax)
plt.savefig('Daily No of Orders by Time', dpi=400, bbox_inches='tight');

"""**Conclusion:**

* Compared between workdays and weekends, order size is not different. However, no of orders daily has big difference. It presented that consumers purchased orders in workdays more than at weekends.
* By time of day, avg number of purchased orders in the afternoon is the highest, compared with other time.

### Analyze cross Day and Time
"""

### Avg orders, order size by day and time ### 

#caculate avg no of orders, order size by day, time:
ord_daytime = products_df.pivot_table(values=["order_id","price"]
                                      , index=['purchase_date','purchase_day','purchase_time']
                                      ,aggfunc={"order_id":'nunique', "price":"sum"})
ord_daytime.fillna(0, inplace = True)
ord_daytime.reset_index(level=['purchase_date','purchase_day','purchase_time'], inplace=True)
ord_daytime["order_size"]=(ord_daytime["price"]/ord_daytime["order_id"])
ord_daytime.rename(columns={'order_id':'no_of_orders'}, inplace=True)

ord_daytime = ord_daytime.pivot_table(values=['no_of_orders','order_size'], index='purchase_time'
                                      , columns='purchase_day'
                                      , aggfunc='mean').astype(int)

#sort columns & index:
day_of_wk = ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday']
ord_daytime = ord_daytime.reindex(index= ['Morning', 'Afternoon','Evening','Night'])
ord_daytime = ord_daytime.reindex(columns= day_of_wk, level = 'purchase_day')

#create heatmap to present data:
cmap = sns.diverging_palette(80,0,90,50, as_cmap=True)
fig, ax = plt.subplots(1,2, figsize=(19,5))
fig.suptitle("Order Analysis by Time and Day", fontsize=15)

sns.heatmap(ord_daytime.iloc[:,:7], cmap=cmap, ax=ax[0], linecolor="white", vmin = 20, vmax = 60, linewidths=0.5
            , annot=True,annot_kws={"size":12})
ax[0].set_title('Avg orders by time in each day of the week')
ax[0].set(xlabel="purchase day")
ax[0].set(ylabel="purchase time")
ax[0].set_xticklabels(day_of_wk, rotation=360)

sns.heatmap(ord_daytime.iloc[:,7:], cmap=cmap, ax=ax[1],linecolor="white", linewidths=0.5
            , annot=True,annot_kws={"size":12},fmt='d')
ax[1].set_title("Order size by time in each day of week")
ax[1].set(ylabel=None, xlabel="purchase day")
ax[1].set_xticklabels(day_of_wk, rotation=360)

format_chart(ax[0])
format_chart(ax[1])

#set fontsize for cbar: use matplotlib.colorbar object, then set labelsize
cbar0 = ax[0].collections[0].colorbar
cbar0.ax.tick_params(labelsize=11)
cbar1 = ax[1].collections[0].colorbar
cbar1.ax.tick_params(labelsize=11)
plt.savefig('Order size by time in each day of week', dpi=400, bbox_inches='tight');

"""**Conclusion:**

* Dig down, the peak time that consumers are more likely to make orders is in the  afternoon of all weekdays, especially Monday to Wednesday.
* On the other hand, consumers consider to purchase high price products in the afternoon of Friday, Saturday and evening of Tuesday.

=> To sum up: the time that retailers can consider for promotion to increase sale volume or launch new products is in afternoon of workdays. However, if retailers consider for promoting high price products, Friday & Saturday’s afternoon may be ideal.

## **3. Order Delivery Performance**

Learn about the delivery performance of the company.

From there, give suggestion for Olist to optimize their delivery services.

### Average Delivered Days
"""

### prepare data support for analysis ###

#add new columns support for analysis
products_df = products_df.assign(
    order_delivered_customer_date = pd.to_datetime(products_df["order_delivered_customer_date"]).dt.date,
    order_delivered_carrier_date = pd.to_datetime(products_df["order_delivered_carrier_date"]).dt.date,
    order_estimated_delivery_date = pd.to_datetime(products_df["order_estimated_delivery_date"]).dt.date,
    shipping_limit_date = pd.to_datetime(products_df["shipping_limit_date"]).dt.date)

products_df = products_df.assign(delivered_days= (products_df['order_delivered_customer_date'] - products_df['purchase_date']).dt.days
                                 ,days_est_vs_deliver= (products_df['order_estimated_delivery_date'] - products_df['order_delivered_customer_date']).dt.days
                                 ,days_limit_vs_deliver_carrier= (products_df['shipping_limit_date'] - products_df['order_delivered_carrier_date']).dt.days)

products_df=products_df.assign(seller_to_carrier=np.where(products_df['days_limit_vs_deliver_carrier']<0,'late deliver to carrier','in time deliver to carrier'))

# create a column with condition: 
products_df['est_to_deliver'] = np.where(products_df['days_est_vs_deliver']<0, 'late deliver', 'on time deliver')

conditions = [(products_df['days_est_vs_deliver'] < -10),
              (products_df['days_est_vs_deliver'] <= -5),
              (products_df['days_est_vs_deliver'] < 0)]
choices = ['late over 10 days', 'late from 5 days to 10 days','late under 5 days']
products_df['est_to_deliver_detail'] = np.select(conditions, choices, default='on time deliver')

#merge with reviews dataset:
#in case, reviews dataset has 2 reviews by orders => get avg score for each order:
reviews_unique = dataframes_dict['reviews'].pivot_table(values='review_score', index='order_id', aggfunc = 'mean')
reviews_unique.reset_index(inplace=True)
products_df=products_df.merge(reviews_unique[['order_id','review_score']], how="left", on ='order_id')

#remove duplicated line for same products in 1 order:
#reason: if customers purchased 2 same products in 1 order, data presented as 2 lines with same delivery information,
#therefore need to remove duplicates before calculate deliver performance
deliver_df=products_df.drop_duplicates(keep=False,inplace=False)

#exam data if any data in col "order_delivered_customer_date", "order_delivered_carrier_date" is NaN
sum_nan = deliver_df.isna().sum()
sum_nan[sum_nan>0]

#remove if any data in col "order_delivered_customer_date", "order_delivered_carrier_date" is NaN
deliver_df=deliver_df.dropna(subset = ['order_delivered_customer_date','order_delivered_carrier_date'])
deliver_df

###Range of delivered days###
#get unique order with its delivered day:
deliver_ord=deliver_df[['order_id','delivered_days']].drop_duplicates(keep=False)

#present data to the chart:
ax = sns.histplot(data=deliver_ord[['order_id','delivered_days']], x='delivered_days', binwidth=5)
ax.set(title='Range of Delivered Day', ylabel='no of orders', xlabel='no of delivered days taken')
ax.set_xlim(0,100)
format_chart(ax)
plt.savefig('Range of Delivered Day', dpi=400, bbox_inches='tight');

###delivered days of top product categories###
#get unique order with its delivered day & categories:
deliver_uni_ord=deliver_df[['order_id','delivered_days','product_category_name_english']].drop_duplicates(keep=False)
deli_top_pro= deliver_uni_ord[deliver_uni_ord['product_category_name_english'].isin(['bed_bath_table', 'health_beauty', 'watches_gifts','sports_leisure','computers_accessories','furniture_decor'])]

#present data to boxplot:
fig, ax = plt.subplots(figsize=(11,6))
sns.boxplot(data=deli_top_pro, x="product_category_name_english", y="delivered_days",order=['health_beauty', 'watches_gifts','bed_bath_table','sports_leisure','computers_accessories','furniture_decor'])
ax.set(title="Delivered days of top product categories", xlabel=None, ylabel='no of delivered days taken')
ax.set_ylim(0,30)
format_chart(ax)
plt.savefig('Delivered days of top product categories', dpi=400, bbox_inches='tight');

"""**Conclusion:**

* Range of delivered day is running up to 100 days due to some outliners. 
* However, most of orders are delivered in range 5 – 15 days from ordered date.
* Dig down by top categories, there is not big difference on delivered days, still around 5 – 15 days.

### Actual Delivery vs Estimation
"""

###Deliver vs estimation###
#calculate no of orders by days variance bw estimation to actual deliver:
dlv_df=deliver_df[['order_id','est_to_deliver']]
dlv=dlv_df.pivot_table(values='order_id',index='est_to_deliver', aggfunc='nunique')
dlv.sort_values(by='order_id', ascending=False, inplace=True)

# present data to chart:
dlv.plot.pie(y='order_id', labels=dlv.index, autopct='%1.1f%%',textprops={'fontsize': 13},figsize=(5, 5))
plt.title('Actual Deliver vs Estimation: %Orders by status', size = 15)
plt.ylabel('')
plt.legend('')
plt.savefig('Actual Deliver vs Estimation: %Orders by status', dpi=400, bbox_inches='tight');

###create df of late deliver orders only ###
late_deliver_df=deliver_df[deliver_df['est_to_deliver']=='late deliver']
late_deli_status=late_deliver_df.pivot_table(values='order_id',index='est_to_deliver_detail', aggfunc='nunique')
late_deli_status.sort_values(by='order_id', ascending=False, inplace=True)

# present data to chart:
late_deli_status.plot.pie(y='order_id', labels=late_deli_status.index, autopct='%1.1f%%',textprops={'fontsize': 13},figsize=(5, 5))
plt.title('% Late Orders by Delayed Days', size = 15)
plt.ylabel('')
plt.legend('')
plt.savefig('Late Orders by Delayed Days', dpi=400, bbox_inches='tight');

###avg reviews score by deliver status###

#select rows not nan in review score and get unique lines by order:
review_score_not_nan = deliver_df[~deliver_df['review_score'].isna()]
uni_review = review_score_not_nan[['order_id','est_to_deliver','review_score']].drop_duplicates()

#create pivot table to get the avg review score of in time delivered orders vs late deliver
uni_review_pv = uni_review.pivot_table(values='review_score', index='est_to_deliver', aggfunc='mean')
uni_review_pv.sort_values(by=['review_score'], ascending=False, inplace=True)

# present data to chart:
fig, ax = plt.subplots(1,1, figsize=(3,5))
ax = sns.barplot(x=uni_review_pv.index, y='review_score', data=uni_review_pv)
ax.set(title="Avg Review Score", xlabel=None, ylabel="review score")


#annotation value in columns:
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 5), 
                   textcoords = 'offset points')
format_chart(ax)
plt.savefig('Avg Review Score', dpi=400, bbox_inches='tight');

###Percentage of late deliver to carrier###

#count unique orders by "est_to_deliver" = "late deliver"
late_deliver=late_deliver_df.pivot_table(values='order_id',index='est_to_deliver', aggfunc='nunique')
late_deliver.rename(columns={'order_id':'unique_orders'}, inplace=True)

#count unique orders by "seller_to_carrier"= "late deliver to carrier"
late_to_carrier=deliver_df[deliver_df['est_to_deliver']=='late deliver'].pivot_table(values='order_id',index='est_to_deliver', columns = 'seller_to_carrier', aggfunc='nunique')
deliver_to_carrier = late_deliver.merge(late_to_carrier, how = "left", on = 'est_to_deliver')

#1 order may have many different products delivered by different sellers to carrier,
#so if only 1 products late deliverd to carrier, we determined that order will be counted as late deliverd to carrier
#that is the reason why need to recalculate no of orders in time delivered to carrier = unique orders - late delivered to carrier orders
deliver_to_carrier.drop(columns='in time deliver to carrier')
deliver_to_carrier['in time deliver to carrier'] = deliver_to_carrier['unique_orders'] - deliver_to_carrier['late deliver to carrier']

deliver_to_carrier.drop(columns='unique_orders',inplace=True)
deli_to_carrier=deliver_to_carrier.T

# present data to chart:
deli_to_carrier.plot.pie(y='late deliver', labels=deli_to_carrier.index, autopct='%1.1f%%',textprops={'fontsize': 13},figsize=(5, 5))
plt.title('%Late vs In-time Deliver to Carrier', size = 15)
plt.ylabel('')
plt.legend('')
plt.savefig('%Late vs In-time Deliver to Carrier', dpi=400, bbox_inches='tight');

"""**Conclusion:**
* Olist has very good delivery performance with only 6.8% late delivery vs estimation. However, in the late delivered orders, over 60% of orders late delivered over 4 days vs estimation. 
* For review score: late deliver orders’ score is just 2.3 while earlier/in-time deliver orders has the score is 4.3. It is big gap.
* Dig down, for late delivery, only  21.7% orders is because sellers late delivered to carrier, nearly 80% come from carrier side to customer. 

=> When Olist business expanded, it is a need for them to care about late delivered orders and measure carefully their carrier’s partners to increase customer satisfaction.

### Red Flag for Late Deliver Seller
"""

### in all orders: which sellers most deliverPercentage of late orders per seller ###
#count unique orders by seller
seller=deliver_df[['seller_id','seller_to_carrier','order_id']]
seller_pv=seller.pivot_table(values='order_id',index='seller_id', aggfunc='nunique')
seller_pv.reset_index(inplace=True)
seller_pv.rename(columns={"order_id": "unique_order"}, inplace=True)

#count late orders by seller
seller_late_deliver=seller[seller.seller_to_carrier=='late deliver to carrier']
seller_late_deli = seller_late_deliver.drop_duplicates()
seller_late=seller_late_deli.pivot_table(values='order_id',index='seller_id', aggfunc='nunique')
seller_late.reset_index(inplace=True)
seller_late.rename(columns={"order_id": "late_order"}, inplace=True)

#calculate %late orders by seller
seller_summary=seller_pv.merge(seller_late, how="left", on='seller_id')
seller_summary.fillna(0, inplace=True)
seller_summary["percent_late_order"]=seller_summary.late_order*100/seller_summary.unique_order
seller_summary.sort_values("percent_late_order", ascending=False, inplace=True)
seller_summary.head(20)

#get avg orders per seller:
avg_ord_per_seller = pd.DataFrame(seller_pv.mean(numeric_only=True))
avg_ord_per_seller.reset_index(inplace=True)

#get seller with most %late order with 2 conditions: no of orders >= avg orders per seller & high %late orders:
seller_top=seller_summary[seller_summary['unique_order']>=avg_ord_per_seller.iloc[0,1]]
seller_top = seller_top.sort_values("percent_late_order", ascending=False)
seller_top_10 = seller_top.head(10)
seller_top_10.set_index('seller_id', inplace=True)

seller_top_10.columns.names = ['']
seller_top_styled = seller_top_10.style.background_gradient()
seller_top_styled

"""**Conclusion:**

* Here is the id list of sellers that have highest percentage of late deliver orders. Olist needs to pay attention to these sellers if they want to increase customer satisfaction.
* Note: only consider no of orders >= avg orders per seller (31 orders within reported period) to reduce the low potential sellers

## **4. RFM Analysis**

In this section we will analyse customer behaviour by segmenting them based on the methodology of RFM.

**RFM (Recency, Frequency and Monetarity)**<br>

### Why RFM analysis should be done?

RFM segmentation will make you able to understand your customer base better, and also serve as a good starting point for your data journey and more advanced customer models. You will be able to give more accurate answers to key questions for your business — for example:

* Who are your best customers?
* Which customers are at the verge of churning?
* Who has the potential to be converted in more profitable customers
* Which customer are lost/inactive?
* Which customers is critical to retain?
* Who are your loyal customers?
* Which group of customers is most likely to respond to your current campaign?

### RFM Basics

RFM is a simple statistical method for categorising customers based on their purchasing behaviour. It is done in 3 steps

**STEP 1 - Calculating the value of each metrics - Recency, Frequency and Monetarity**<br>
The behaviour is identified by using only three customer data points:

1. Recency(R) - The freshness of customer activity - This is calculated for a customer by finding out `time or number of days since last purchase`
2. Frequency(F) - The frequency of customer transactions - This is calculated for a customer by finding out `total number of transactions`
3. Monetarity(M) - The willingness to spend - This is calculated by finding out `total transaction value`

For eg. Misa is a customer who has made the last purchase 7 days ago, the total number of transaction done by her is 100 and the totol transaction value is $1000, based on above information following RFM table or dataframe is built

| Customer     | Recency (R)  | Frequency (F) | Monetarity (M) |
|--------------|--------------|---------------|----------------|
|Misa          |       7      |     100       |     1000       |

**STEP 2 - Assigning a score for each metrics - R , F and M score**<br> 
Now that we have the values ​​of each of the RFM letters, it's time to assign a score . The score ranges from 1 to 5, where the higher this number, the better. This score is assigned to each of the letters, that is, R will have a score, F will have another and M too, which may or may not be the same — for example, all letters may receive a score of 5.

The scoring is usually done using the method of quantile based descritisation which will be shown later.

For eg. for Misa the scores after applying the quantile based descritisation the score might come like below

| Customer     | Recency (R)  | Frequency (F) | Monetarity (M) | R | F | M |
|--------------|--------------|---------------|----------------|---|---|---|
|Misa          |       7      |     100       |     1000       | 5 | 5 | 5 |


**STEP 3 - Segment the customer based on the score**<br>
Based on the scores `Mean of F and M score` and separate `R score` the customer can be divided into following segments

| Segment Name               | Range of R values | Range of F and M Average |
|----------------------------|-------------------|--------------------------|
| Champions                  |       4 - 5       |           4 - 5          |
| Loyal Customers            |       2 - 5       |           3 - 5          |
| Potential Loyalist         |       3 - 5       |           1 - 3          |
| New Customers              |       4 - 5       |           0 - 1          |
| Promising                  |       3 - 4       |           0 - 1          |
| Customers Needing Attetion |       2 - 3       |           2 - 3          |
| About to Sleep             |       2 - 3       |           0 - 2          |
| At Risk                    |       0 - 2       |           2 - 5          |
| Can't Lose Them            |       0 - 1       |           4 - 5          |
| Hibernating                |       1 - 2       |           1 - 2          |
| Lost                       |       0 - 2       |           0 - 2          |

The meaning of each segment is shown below<br>
**Champions:** These are customers who have bought recently, buy often, and spend a lot.

**Loyal Customers:** These are customers who spend well and often.

**Potential Loyalist:** These are recent buyers, spend a good amount and have bought more than once.

**New Customer:** These are customer who have bought recently but they are new so their spending is less and have bought very less.

**Promising:** These are customer who have bought recently but not as recently as new customer and their spending is less and have bought very less.

**Customer Needing Attetion:** These are customers who have recently purchased, however are still in doubt whether they will make their next purchase from the company or a competitor.

**About to Sleep:** These are customers who haven't bought in a long time, but may still buy again.

**At Risk:** These are customers who have spent very little money and buy frequently, but have not bought for a long time

**Can't lose them:** Used to purchase frequently but haven’t returned for a long time.

**Hibernating:** These are customers who have bought a long time ago, only a few times and have spent little

**Lost:** These are customers who have bought a long time ago, only a few times and have spent even more little than hibernating customers. 

For eg. based on Misa's score she belongs to Champions category

| Customer     | Recency (R)  | Frequency (F) | Monetarity (M) | R | F | M |FM-mean| Segment |
|--------------|--------------|---------------|----------------|---|---|---|-------|---------|
|Misa          |       7      |     100       |     1000       | 5 | 5 | 5 | 5      |Champions|

After segmenting we will propose possible CRM (customer relationshop management) strategy for each segment of customer. CRM strategy is nothing but what kind of sales strategy we can apply to retain customers.

For eg. for Misa we should be rewarding her by more offers and ask for her reviews which could help us to get more customer later on.

### RFM calculation

#### Preparing the data

We already have the products_df dataset with all the columns available below
"""

products_df.head()

products_df.columns

"""For RFM analysis, we will use the following columns
1. customer_unique_id - to identify each customer
2. price - to calculate monetarity value
3. purchase_date - to calculate recency value
2. order_id - to calculate frequency of purchase for each customers


"""

rfm_pre_data = products_df[['customer_unique_id','price','purchase_date','order_id']]
rfm_pre_data

"""#### Step 1 - calculating RFM values

Now we will calculate the values for Recency, Frequency and Monetarity using the groupby operation on customer unique id column.

* Monetarity - sum of price column for each customer
* Frequency - count of unique order_id per customer
* Recency - Difference between date of cusotmer's last purchase (this is basically the maximum purchase date per customer) and Maximum date of the dataset

Maximum date of the dataset - the most recent purchase plus 1 day
"""

max(rfm_pre_data['purchase_date'])

import datetime
max_date = max(rfm_pre_data['purchase_date']) + datetime.timedelta(days=1)

"""We can calculate recency and monetarity using rfm_predata as shown below"""

rec_mon_data = rfm_pre_data.groupby('customer_unique_id').agg({
    'purchase_date': lambda x: (max_date - x.max()).days,
    'price':'sum'
}).reset_index()

rec_mon_data

rec_mon_data.columns =['customer_unique_id','recency','monetarity']

rec_mon_data

"""The calculation of frquency or number of purchases per customer_unique_id needs understanding of below Note

Note:<br>
* Since multiple items were purchased on one order_id, so when all the orders and order_items datasets were merged the same order_id would be repeated multiple times. 
* When this merged dataset was merged with customers dataset then the same order_id will be repeated for each customer_unique_id. 
* This means that when calculating number of purchases by each customer just counting the number of order_id would not be enough becuase the same order_id will be repeated multiple times for each customer. 
* We actually need number of unique order_id per customer to calculate number of purchases or frequency

We will achieve the above task by just taking the customer_unique_id and order_id columns and then drop_duplicate rows so that we will get only single order_id and customer_unique_id mapping. 
"""

rfm_freq_data = rfm_pre_data[['customer_unique_id','order_id']]
rfm_freq_data.shape

rfm_freq_data_uni = rfm_freq_data.drop_duplicates()
rfm_freq_data_uni.shape

"""So you can see from above that order_id were repeated for each customer_unique_id and by doing de-duplication we got unique pairs of order_id and customer_unique_id"""

rfm_freq = rfm_freq_data_uni.groupby('customer_unique_id')['order_id'].count().reset_index()
rfm_freq

rfm_freq.columns = ['customer_unique_id','frequency']
rfm_freq

"""Now we will combine rfm_freq and rec_mon_data"""

rfm_data = rfm_freq.merge(rec_mon_data, on = 'customer_unique_id',how='inner')
rfm_data

"""Before calculating the RFM scores we will analyse the distribution of Frequency, Recency and Monetarity"""

rfm_data.describe()

sns.distplot(rfm_data['recency'])

sns.distplot(rfm_data['frequency'])

sns.distplot(rfm_data['monetarity'])

# number of customer who made more than one purchase
print(rfm_data[rfm_data['frequency']>1].shape[0] / rfm_data.shape[0])

"""Conclusion:<br>
* the frequency of customers is the big problem for the in RFM analysis, because only 3% of customers made more than one purchase but is something that's very common in ecommerce in real life.
* An average monetary customer is R\$141 and 75% of the customers spend less than R\$154 and the maximum was R\\$13440.
* we can see the 25% of customers have a recency of 3.9 months (or 115 days) with an average of 8 months(or 238 days). With a purchase frequency equivalent to 1 and with this recency this indicates that customers make very specific purchases.

#### Step 2 - Assigning RFM scores

Now we will assign the score of 1-5 for each metric.

Higher the score the better the metric.

The score will be done based on quantile based descritisation technique.

Let's try to understand it through an example.

Let's suppose wee have 10 customer with following monetarity values

| cust_id | monetarity |
|---------|------------|
|  1      |    10      |
|  2      |    20      |
|  3      |    30      |
|  4      |    40      |
|  5      |    50      |
|  6      |    60      |
|  7      |    70      |
|  8      |    80      |
|  9      |    90      |
| 10      |   100      |

We will first sort the data based on monetarity value (this is already done above) and then create 5 equal buckets based on the calculation of percentiles from monetarity data. 

So there will be 5 percentile buckets with equal customer in each. 1st percentile bucket will be from 0 percentile to 20 percentile and will contain customer 1 and 2. This will be done for each customer.

The bucket with lowest monetarity values will be given score 1 and the highest one will be given 5

| cust_id | monetarity | M |
|---------|------------|---|
|  1      |    10      | 1 |
|  2      |    20      | 1 |
|  3      |    30      | 2 |
|  4      |    40      | 2 |
|  5      |    50      | 3 |
|  6      |    60      | 3 |
|  7      |    70      | 4 |
|  8      |    80      | 4 |
|  9      |    90      | 5 |
| 10      |   100      | 5 |

**Calculate the R score**<br>
For recency, customers with a lower recency are more valuable than those with a higher recency. For example, someone who shopped 7 days ago is far more likely to be a customer than someone last seen 1298 days ago. As a result, we score these differently to the frequency and monetary labels. To create five approximately equal segments of customers we’ll apply qcut() to the recency column and assign labels from 5 to 1.
"""

rfm_data['r'] = pd.qcut(rfm_data['recency'], q=5, labels=[5, 4, 3, 2, 1])

rfm_data

"""Let's try to check the distribution of r scores in each scoring category from 1 to 5"""

rfm_data.groupby('r').agg(
    count=('customer_unique_id', 'count'),
    min_recency=('recency', min),
    max_recency=('recency', max),
    std_recency=('recency', 'std'),    
    avg_recency=('recency', 'mean')
).sort_values(by='avg_recency')

"""**Calculate the M scores**<br>
Since a higher monetary value is a better thing for the business, we score customers here from 1 to 5. Those who spend the most get a five and those who spend the least get a 1. We can use the same approach for qcut() and just switch the direction of the labels.

"""

rfm_data['m'] = pd.qcut(rfm_data['monetarity'], q=5, labels=[1, 2, 3, 4, 5])
rfm_data

"""Let's try to check the distribution of m scores in each scoring category from 1 to 5"""

rfm_data.groupby('m').agg(
    count=('customer_unique_id', 'count'),
    min_monetary=('monetarity', min),
    max_monetary=('monetarity', max),
    std_monetary=('monetarity', 'std'),
    avg_monetary=('monetarity', 'mean')
).sort_values(by='avg_monetary')

"""**Calculate the F scores**<br>

Using qcut() quantile-based discretization to create frequency scores is more challenging because the typical spread of order volumes means that bin edges may overlap. This is because 97% of customer have done only 1 purchase so dividing equal number of customer in 5 buckets will create overlap. 

If you experience this, qcut() will throw an error telling you “ValueError: Bin edges must be unique” and that you can “You can drop duplicate edges by setting the ‘duplicates’ kwarg”. You can avoid this by appending .rank(method='first').
"""

rfm_data['f'] = pd.qcut(rfm_data['frequency'], q=5, labels=[1, 2, 3, 4, 5])

rfm_data['f'] = pd.qcut(rfm_data['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5])
rfm_data.head()

"""Since frequency data is skewed, so quantile based discretisation gives you lovely equal group sizes, but the scores don’t make a lot of sense because the frequency is heavily skewed resulting in a really long and quite useless tail.

We can observe this by checking the distribution of f scores in each scoring category from 1 to 5
"""

rfm_data.groupby('f').agg(
    count=('customer_unique_id', 'count'),
    min_frequency=('frequency', min),
    max_frequency=('frequency', max),
    std_frequency=('frequency', 'std'),
    avg_frequency=('frequency', 'mean')
).sort_values(by='avg_frequency')

"""Finally all the scores are concatenated to form an aggregate score called `rfm` score.

"""

rfm_data['rfm'] = rfm_data['r'].astype(str) +\
                      rfm_data['f'].astype(str) +\
                      rfm_data['m'].astype(str)

rfm_data

"""Now the customer with the aggregate score of `555` is the best customer.

#### Step 3 - Segmenting customers

Now we will use the below function to segment our customer into 11 categories as taught in RFM basics section.
"""

def get_segment(data):
    mean_fm = (data['f'] + data['m']) / 2
    
    if (data['r'] >= 4 and data['r'] <= 5) and (mean_fm >= 4 and mean_fm <= 5):
        return 'Champions'
    if (data['r'] >= 2 and data['r'] <= 5) and (mean_fm >= 3 and mean_fm <= 5):
        return 'Loyal Customers'
    if (data['r'] >= 3 and data['r'] <= 5) and (mean_fm >= 1 and mean_fm <= 3):
        return 'Potential Loyslist'
    if (data['r'] >= 4 and data['r'] <= 5) and (mean_fm >= 0 and mean_fm <= 1):
        return 'New Customers'
    if (data['r'] >= 3 and data['r'] <= 4) and (mean_fm >= 0 and mean_fm <= 1):
        return 'Promising'
    if (data['r'] >= 2 and data['r'] <= 3) and (mean_fm >= 2 and mean_fm <= 3):
        return 'Customer Needing Attention'
    if (data['r'] >= 2 and data['r'] <= 3) and (mean_fm >= 0 and mean_fm <= 2):
        return 'About to Sleep'
    if (data['r'] >= 0 and data['r'] <= 2) and (mean_fm >= 2 and mean_fm <= 5):
        return 'At Risk'
    if (data['r'] >= 0 and data['r'] <= 1) and (mean_fm >= 4 and mean_fm <= 5):
        return "Can't Lose Then"
    if (data['r'] >= 1 and data['r'] <= 2) and (mean_fm >= 1 and mean_fm <= 2):
        return 'Hibernating'
    return 'Lost'

rfm_data['segment'] = rfm_data.apply(get_segment,axis=1)

plt.figure(figsize=(10,5))
percentage = (rfm_data['segment'].value_counts(normalize=True)* 100).reset_index(name='percentage')
g = sns.barplot(x=percentage['percentage'],y=percentage['index'], data=percentage,palette="GnBu_d")
sns.despine(bottom = True, left = True)
for i, v in enumerate(percentage['percentage']):
    g.text(v,i+0.20,"  {:.2f}".format(v)+"%", color='black', ha="left")
g.set_ylabel('Segmentation')
g.set(xticks=[])
plt.show()

"""### CRM strategy for different customer segments

**24.66% - Potential Loyslist:**

These are recent buyers, spend a good amount and have bought more than once.

<u>CRM Strategy</u>:

* Offer a loyalty program;
* Keep them engaged;
* Personalized and other product recommendations.

**2.46% - About to Sleep:**

These are customers who haven't bought in a long time, but may still buy again.

<u>CRM Strategy</u>:

* Offer discounts;
* Recommendation of popular products.

**2.66% - Hibernating:**

These are customers who have bought a long time ago, only a few times and have spent little

<u>CRM Strategy</u>:

* Standard communication for sending offers;
* Offer relevant products and good deals.

**5.62% - Customer Needing Attetion:**

These are customers who have recently purchased, however are still in doubt whether they will make their next purchase from the company or a competitor.

<u>CRM Strategy</u>:

* Promotional campaigns for a limited time;
* Product recommendations based on their behavior;
* Show the importance of buying with the company.

**17.26% - At Risk:**

These are customers who have spent very little money and buy frequently, but have not bought for a long time

<u>CRM Strategy</u>:

* Send personalized communications and other messages to reconnect;
* Offer good deals.

**37.44% - Loyal Customers:**

These are customers who spend well and often.

<u>CRM Strategy</u>:

* Personalized communication;
* Avoid mass mailing of offers;
* Offer few products, but present products that they are likely to be interested in;
* Ask for product reviews.

**9.89% - Champions:**

These are customers who have bought recently, buy often, and spend a lot.

<u> CRM Strategy</u>:

* Special offers, products and discounts for these customers so they feel valued;
* Ask for reviews and feedbacks constantly;
* Avoid sending massive amounts of offers;
* Personalized communication;
* Give rewards.
"""

